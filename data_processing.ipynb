{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import trange, tqdm\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "import csv\n",
    "import unicodedata\n",
    "import html\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "from numpy import ndarray \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nptyping import NDArray, Int, Shape\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "import ssl\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "from piazza_api import Piazza\n",
    "from piazza_api.network import Network\n",
    "\n",
    "CRED_FILE = \"creds.json\"\n",
    "NUM_POSTS_TO_SCRAPE = 1000\n",
    "\n",
    "\n",
    "\"\"\"Custom Types\"\"\"\n",
    "Answer = Dict[str,Dict[str,Union[str,int]]]\n",
    "Post = Dict[str,Union[str, Union[str,int,List]]]\n",
    "\n",
    "\"\"\"Macros\"\"\"\n",
    "# who the answer is coming from\n",
    "STUDENT, INSTRUCTOR, STUDENT_ENDORSED_ANSWERER = 0, 1, 2\n",
    "EPSILON = 1e-05\n",
    "\n",
    "# folder categories\n",
    "GENERAL, LECTURES, ASSIGNMENTS, TESTS = 0, 1, 2, 3\n",
    "\n",
    "# labels for sentiment\n",
    "NEGATIVE, NEUTRAL, POSITIVE = 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHTMLParser(HTMLParser):\n",
    "    \"\"\"taken from: [1]\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    \"\"\"strips html tags and substitutes html entities \"\"\"\n",
    "    #html = html.unescape(html)\n",
    "    s =  MyHTMLParser()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login() -> Tuple[dict, Network]:\n",
    "    \"\"\"logs user into Piazza\"\"\"\n",
    "\n",
    "    email:str \n",
    "    password:str \n",
    "    courseid:str \n",
    "\n",
    "    with open(CRED_FILE) as f:\n",
    "        creds = json.load(f)\n",
    "        email, password, courseid = creds['email'], creds['password'], creds['courseid']\n",
    "\n",
    "\n",
    "    #print(f\"email: {email} \\npassword: {password} \\ncourseid: {courseid}\")\n",
    "\n",
    "\n",
    "    p: Piazza = Piazza()\n",
    "    p.user_login(email, password)\n",
    "    user_profile: dict = p.get_user_profile()\n",
    "    course: Network = p.network(courseid)\n",
    "    return user_profile, course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_creator(post: Post):\n",
    "    for entry in post['change_log']:\n",
    "        if entry['type'] == 'create':\n",
    "            return entry['uid']\n",
    "\n",
    "\n",
    "def get_post_created(post: Post):\n",
    "    \"\"\"get time post was created\"\"\"\n",
    "    for entry in post['change_log']:\n",
    "        if entry['type'] == 'create':\n",
    "            return entry['when']\n",
    "\n",
    "\n",
    "def get_posts_by_student(filename:str, student_id:str) -> List[Post]:\n",
    "    student_posts = []\n",
    "    with open(filename, 'r') as f:\n",
    "        all_posts = json.load(f)\n",
    "        for p in all_posts:\n",
    "            if get_post_creator(p) == student_id:\n",
    "                student_posts.append(p)\n",
    "    return student_posts\n",
    "\n",
    "\n",
    "def get_endorsed_students(course: Network) -> Tuple[Dict, Dict]:\n",
    "    endorsed_users = {}\n",
    "    non_endorsed_users = {}\n",
    "    users = course.get_all_users()\n",
    "    for u in users:\n",
    "        if u['endorser']:\n",
    "            endorsed_users[u['id']] = u['name']\n",
    "        else:\n",
    "            non_endorsed_users[u['id']] = u['name']\n",
    "\n",
    "\n",
    "    return endorsed_users, non_endorsed_users\n",
    "\n",
    "\n",
    "def is_private(post: Post) -> bool:\n",
    "    \"\"\" Return true if post is private \"\"\"\n",
    "    for entry in post['change_log']:\n",
    "        if entry['type'] == 'create':\n",
    "            return True if entry['v'] == 'private' else False\n",
    "\n",
    "\n",
    "\n",
    "def get_answers(post:Post, endorsed_students: Dict) -> List[Dict[str, Answer]]:\n",
    "    \"\"\" Get student and instructor answers \"\"\"\n",
    "\n",
    "    answers = {}\n",
    "    answers['s_answer'] = {}\n",
    "    answers['i_answer'] = {}\n",
    "\n",
    "    for t in answers.keys():\n",
    "        for ans in post['children']:\n",
    "            if ans['type'] == t:      \n",
    "                vals = answers[t]\n",
    "                text = ans['history'][0]['content']\n",
    "                #text = strip_tags(text)\n",
    "                vals['text'] = text\n",
    "                vals['poster'] = ans['history'][0]['uid']\n",
    "                vals['date'] = ans['history'][0]['created']\n",
    "                vals['num_helpful'] = len(ans['tag_endorse_arr'])\n",
    "                # post creator is same student that liked response\n",
    "                if get_post_creator(post) in ans['tag_endorse_arr']:\n",
    "                    vals['is_helpful'] = True \n",
    "                else:\n",
    "                    vals['is_helpful'] = False\n",
    "\n",
    "                if ans['type'] == \"s_answer\":\n",
    "                    \n",
    "                    student_poster_id = ans['history'][0]['uid'] # id of the most recent student answer editor\n",
    "                     # check if student is endorsed (actually not a valid way of checking)\n",
    "                    vals['is_endorser'] = False\n",
    "                    if student_poster_id in endorsed_students:\n",
    "                        vals['is_endorser'] = True\n",
    "                   \n",
    "                break\n",
    "    \n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Convert raw Piazza data to csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_posts_json(filename:str, course:Network) -> None:\n",
    "    \"\"\"Create json of all posts saved in current directory\"\"\"\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{filename} already exists!\")\n",
    "        return \n",
    "    posts = course.iter_all_posts()\n",
    "    all_posts = []\n",
    "    #text = json.dumps(post['children'][1], sort_keys=True, indent=4)\n",
    "    try:\n",
    "        for p in tqdm(posts):\n",
    "            time.sleep(1)\n",
    "            all_posts.append(p)\n",
    "    \n",
    "    finally:\n",
    "        print('------------------------------------')\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(all_posts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def json_to_csv(json_file_path: str, csv_filename: str, course: Network, is_overwrite_csv: bool=False) -> None:\n",
    "    \"\"\" \n",
    "    :param json_file_path: Path to json file to convert to csv\n",
    "    :param csv_filename: Name of csv file to save to cur directory\n",
    "    :param course: Used to extract student profile to determine whether they are endorsed. **Actually not a valid way of checking**\n",
    "    \"\"\"\n",
    "\n",
    "    schema = (\"post_id,is_private,question_title,question,folders,student_poster_name,date_question_posted,\" \n",
    "    \"student_answer,student_answer_name,date_student_answer_posted,is_student_endorsed,is_student_helpful,\"\n",
    "    \"instructor_answer,instructor_answer_name,date_instructor_answer_posted,is_instructor_helpful,\" \n",
    "    \"is_followup\\n\")\n",
    "\n",
    "    parser = MyHTMLParser()\n",
    "\n",
    "    endorsed_students = get_endorsed_students(course)[0]\n",
    "\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        with open(csv_filename, 'w') as csv_file:\n",
    "            csv_file.write(schema)\n",
    "            posts = json.load(json_file)\n",
    "            num_posts = 0\n",
    "            for post in tqdm(posts):   \n",
    "                row = [] \n",
    "                if post['type'] == 'question':\n",
    "                    question = post['history'][0] # newest update of question. Change index to -1 for oldest\n",
    "                    question_title = question['subject']\n",
    "                    question_content = question['content']\n",
    "                    folders = ','.join(post['folders'])\n",
    "                    date_created = get_post_created(post)\n",
    "                    answers = get_answers(post, endorsed_students)\n",
    "                    student_answer = answers['s_answer']\n",
    "                    instructor_answer = answers['i_answer']\n",
    "                 \n",
    "\n",
    "                    row = [post['nr'], is_private(post), question_title, question_content, folders, get_post_creator(post), date_created]\n",
    "                    s_row, i_row = [], []\n",
    "                    if student_answer:\n",
    "                        s_row = [student_answer['text'], student_answer['poster'], student_answer['date'], str(student_answer['is_endorser']), str(student_answer['is_helpful'])] \n",
    "                    else:\n",
    "                        s_row = [None, None, None, None, None]\n",
    "\n",
    "                    if instructor_answer:\n",
    "                        i_row = [instructor_answer['text'], instructor_answer['poster'], instructor_answer['date'], str(instructor_answer['is_helpful'])] \n",
    "                    else:\n",
    "                        i_row = [None, None, None, None]\n",
    "                    \n",
    "                    row = row + s_row + i_row\n",
    "\n",
    "                    is_followup = 'False'\n",
    "\n",
    "                    for c in post['children']:\n",
    "                        if c['type'] == 'followup':\n",
    "                            is_followup = 'True'\n",
    "                    \n",
    "                    row += [is_followup]\n",
    "\n",
    "                    post_writer = csv.writer(csv_file)\n",
    "                    post_writer.writerow(row)\n",
    "                    \n",
    "                    csv_file.write('\\n')\n",
    "\n",
    "                    num_posts += 1\n",
    "\n",
    "                    if num_posts == NUM_POSTS_TO_SCRAPE:\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 871/871 [00:00<00:00, 59546.52it/s]\n"
     ]
    }
   ],
   "source": [
    "user_profile,course = login()\n",
    "\n",
    "#export_posts_json(\"csc108_fall2020.json\", course)\n",
    "json_to_csv(\"./csc108_fall2020.json\", \"csc108_fall2020.csv\", course)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually viewing users and posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_profile,course = login()\n",
    "\n",
    "# user = course.get_users(['krz7jwkviui2p3'])\n",
    "# post = course.get_post('3809')\n",
    "\n",
    "# p = course.get_feed()\n",
    "\n",
    "# text = json.dumps(post, sort_keys=True, indent=4)\n",
    "\n",
    "\n",
    "# users = course.get_all_users()\n",
    "\n",
    "# user\n",
    "\n",
    "# print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to handle posts with imgs? Do we want the img tags stripped? Think about how it will affect textual features\n",
    "response length, sentiment, \n",
    "\n",
    "what elements do q&a contain?\n",
    "latex, code snippets, imgs/screenshots, links, lists, annotations to prev posts (i.e. @356)\n",
    "\n",
    "fields that can be added: num_answer_imgs, ...\n",
    "\n",
    "can remove posts with imgs or include a special field called \"num_imgs\" so can distinguish b/w posts that have imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering - Transform csv into another csv with relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH_CSV = \"./csc108_fall2021.csv\"\n",
    "AUGMENTED_FILEPATH_CSV = \"./csc108_fall2021_sentiment_aug.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1101"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(FILEPATH_CSV, index_col=0)\n",
    "data.tail()\n",
    "\n",
    "len(data[data['is_private'] == True])\n",
    "len(data[data['is_private'] == False])\n",
    "\n",
    "# data.keys()\n",
    "# students = data[data[' is_student_endorsed'] == True]['student_answer_name']\n",
    "# for s in students:\n",
    "#     print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New schema: (post_id,student_poster_id,close_to_deadline, \"is_followup\"\" \n",
    "    \n",
    "    \"answerer_id, date_answer_posted, reputation, is_helpful\"\n",
    "    \n",
    "    )\n",
    "\n",
    "\n",
    "    date_question_posted\n",
    "    - close to deadline: yes/no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jaipers5/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(text:str) -> int:\n",
    "    length = 0\n",
    "    if isinstance(text, str):\n",
    "        length = len(word_tokenize(text))\n",
    "    else: # must be nan\n",
    "        if not isnan(text):\n",
    "            assert(1 == 0) # shouldn't get here\n",
    "    \n",
    "    return length\n",
    "\n",
    "\n",
    "def is_references(text: str) -> bool:\n",
    "    \"\"\" Check if answer contains a link to another post (i.e. @256) or a hyperlink\n",
    "        :param text: question|answer with html stripping \n",
    "    \"\"\"   \n",
    "    return True if re.search(r'@+\\d', text) or 'http' in text else False\n",
    "\n",
    "def level_of_detail(text: str) -> bool:\n",
    "    \"\"\" Detect imgs or code snippets\n",
    "        :param text: raw question|answer without html stripping so can detect imgs/code-snippets\n",
    "    \"\"\"\n",
    "    is_image = True if '<img' in text else False\n",
    "    is_code_snippets = True if '<pre' in text else False\n",
    "\n",
    "    return True if is_image or is_code_snippets else False\n",
    "\n",
    "\n",
    "def answer_response_time(t1:str, t2:str) -> int:\n",
    "    \"\"\":returns: Answer response time in mins, rounded up. Add option to use log scale?\"\"\"\n",
    "    d1 = datetime.fromisoformat(t1[:-1])\n",
    "    d2 = datetime.fromisoformat(t2[:-1])\n",
    "    delta = d2-d1\n",
    "    response_time = math.ceil(delta.total_seconds() // 60)\n",
    "    if response_time == 0:\n",
    "        response_time = EPSILON\n",
    "    return response_time\n",
    "    \n",
    "def get_category(folder: str, folder_set: dict[str, int]) -> int:\n",
    "    \"\"\" For multi-categories just choose the 1st one.\n",
    "        Varies b/w classes.\n",
    "\n",
    "        :param folder: folder1,folder2, ...\n",
    "        :param folder_set: set of all folders\n",
    "\n",
    "        Precondition: folder should be part of folder_set\n",
    "\n",
    "    \"\"\"\n",
    "    category = folder.split(',')[0]\n",
    "    if category not in folder_set:\n",
    "        assert(1 == 0)\n",
    "\n",
    "    return folder_set[category]\n",
    "\n",
    "\n",
    "def get_sentiment(text: str, sentiment_model) -> int:\n",
    "    senti = sentiment_model(text)[0]['label']\n",
    "    if senti == 'LABEL_0':\n",
    "        return NEGATIVE\n",
    "    elif senti == 'LABEL_1':\n",
    "        return NEUTRAL\n",
    "    else:\n",
    "        return POSITIVE\n",
    " \n",
    "\n",
    "    \n",
    "\n",
    "def add_answer(augmented_data:List[List], append_row:List, post_row:tuple, poster_dict: Dict[str, str], num_instances:int, answer_type:int,\n",
    "              add_nlp_features=True, sentiment_model=None) -> int:\n",
    "    \"\"\"\n",
    "    Append student or instructor answer fields to augmented_data.\n",
    "\n",
    "    :param augmented_data: table to add append_row to\n",
    "    :param append_row: partially filled row to be completed\n",
    "    :param post_row: namedtuple containing information about the current Piazza post\n",
    "    :param answer_type: INSTRUCTOR|STUDENT\n",
    "    :returns: this is a description of what is returned\n",
    "    :raises Nothing\n",
    "    \"\"\"\n",
    "    poster =  'student' if answer_type == STUDENT  else 'instructor'\n",
    "    fields = [f'{poster}_answer', f'{poster}_answer_name', f'is_{poster}_helpful', 'date_{poster}_answer_posted']\n",
    "    increment_num_instances = False\n",
    "    \n",
    "    answer = getattr(post_row, f\"{poster}_answer\")\n",
    "   \n",
    "    if isinstance(answer, str): # if answer is nan, don't add an entry\n",
    "        stripped_answer = strip_tags(answer)\n",
    "        poster_id = getattr(post_row, f'{poster}_answer_name')\n",
    "        if poster_id not in poster_dict:\n",
    "            poster_dict[poster_id] = num_instances \n",
    "            increment_num_instances = True\n",
    "\n",
    "        is_helpful = 1 if getattr(post_row, f\"is_{poster}_helpful\") else 0\n",
    "\n",
    "        \n",
    "        #append_row.append(stripped_answer)\n",
    "        append_row.append(poster_dict[poster_id])\n",
    "        append_row.append(get_length(stripped_answer))\n",
    "        append_row.append(is_references(stripped_answer))\n",
    "        append_row.append(level_of_detail(answer))\n",
    "        \n",
    "        if add_nlp_features:\n",
    "            append_row.append(get_sentiment(stripped_answer, sentiment_model))\n",
    "\n",
    "        t1 = getattr(post_row, 'date_question_posted')\n",
    "        t2 = getattr(post_row, f'date_{poster}_answer_posted')\n",
    "        response_time = answer_response_time(t1, t2)\n",
    "\n",
    "\n",
    "        append_row.append(response_time)\n",
    "        \n",
    "        append_row.append(answer_type)\n",
    "        append_row.append(is_helpful)\n",
    "        augmented_data.append(append_row)\n",
    "\n",
    "    return increment_num_instances\n",
    "\n",
    "    \n",
    "#'general,lecture'.split(',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_sentiment()\n",
    "# a = \"Cats\"\n",
    "# #get_sentiment(a, sentiment_pipeline)\n",
    "\n",
    "# sentiment_pipeline(a)[0]['label']\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# text = \"Good night ðŸ˜Š\"\n",
    "# encoded_input = tokenizer(a, return_tensors='pt', truncation=True, max_length=512)\n",
    "# output = model(**encoded_input)\n",
    "# scores = output[0][0].detach().numpy()\n",
    "#scores = softmax(scores)\n",
    "#scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5836b568fb94971aac863e2f12e53e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "sentiment_pipeline = pipeline(model=MODEL, max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add non-textual and textual features\n",
    "\n",
    "Note that the NLP features take a while to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isnan\n",
    "\n",
    "def augment_data(data: DataFrame, folder_set: dict[str, int], add_nlp_features=True, sentiment_model=None, save_to_csv=True) -> DataFrame:\n",
    "\n",
    "    augmented_data = []\n",
    "\n",
    "    studentid_to_int = {}\n",
    "    instructorid_to_int = {}\n",
    "    num_users = 0\n",
    "    #num_students, num_instructors = 0, 0\n",
    "\n",
    "    num_iters = 0\n",
    "    #num_iters_thresh = 10  # len(data.index)\n",
    "    num_iters_thresh = len(data.index)\n",
    "\n",
    "    #sentiment_pipeline = pipeline(model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "    for r in tqdm(data.itertuples(), total=num_iters_thresh, colour='green', desc=\"Augment csv data\"):\n",
    "\n",
    "        if num_iters > num_iters_thresh:\n",
    "            break\n",
    "\n",
    "       \n",
    "        new_row = [r.Index] \n",
    "        new_row.append(r.is_private)\n",
    "        category = get_category(r.folders, folder_set)\n",
    "        new_row.append(category)\n",
    "       \n",
    "        if r.student_poster_name not in studentid_to_int:\n",
    "            studentid_to_int[r.student_poster_name] = num_users \n",
    "            num_users += 1\n",
    "            \n",
    "        new_row.append(studentid_to_int[r.student_poster_name])\n",
    "        stripped_q = \"\"\n",
    "        raw_question = \"\"\n",
    "        question_sentiment = NEUTRAL\n",
    "        if isinstance(r.question, str): \n",
    "            stripped_q = strip_tags(r.question)\n",
    "            raw_question = r.question\n",
    "            if add_nlp_features:\n",
    "                question_sentiment = get_sentiment(stripped_q, sentiment_model) # strip down to 512 tokens\n",
    "\n",
    "        #new_row.append(stripped_q)\n",
    "        new_row.append(get_length(stripped_q))\n",
    "        new_row.append(is_references(stripped_q))\n",
    "        new_row.append(level_of_detail(raw_question))\n",
    "\n",
    "        if add_nlp_features:\n",
    "            new_row.append(question_sentiment)\n",
    "\n",
    "\n",
    "    \n",
    "        is_followup = 1 if r.is_followup else 0\n",
    "        new_row.append(is_followup)\n",
    "\n",
    "        # add separate rows for student and instructor answer\n",
    "        num_users += add_answer(augmented_data, deepcopy(new_row), r, studentid_to_int, num_users, STUDENT, add_nlp_features, sentiment_model)\n",
    "        num_users+= add_answer(augmented_data, deepcopy(new_row), r, instructorid_to_int, num_users, INSTRUCTOR, add_nlp_features, sentiment_model)\n",
    "        num_iters += 1\n",
    "    \n",
    "    augmented_data = np.array(augmented_data)\n",
    "    print(augmented_data.shape)\n",
    "    if add_nlp_features:\n",
    "        augmented_df = pd.DataFrame(augmented_data, columns=['post_id', 'is_private', 'category', 'student_poster_id', \n",
    "        'question_length', 'is_question_references', 'question_lod', 'question_sentiment', 'is_followup', \n",
    "        'answerer_id', 'answer_length', 'is_answer_references', 'answer_lod', 'answer_sentiment', 'response_time',  'reputation', 'is_helpful'])\n",
    "    else:\n",
    "        augmented_df = pd.DataFrame(augmented_data, columns=['post_id', 'is_private', 'category', 'student_poster_id', \n",
    "        'question_length', 'is_question_references', 'question_lod', 'is_followup', \n",
    "        'answerer_id', 'answer_length', 'is_answer_references', 'answer_lod', 'response_time',  'reputation', 'is_helpful'])\n",
    "\n",
    "    if save_to_csv:\n",
    "        augmented_df.index.name = 'ID'\n",
    "        augmented_df.to_csv(AUGMENTED_FILEPATH_CSV)\n",
    "\n",
    "\n",
    "    return augmented_df\n",
    "\n",
    "\n",
    "csc108_fall2021_categories = {'general': GENERAL, 'lecture': LECTURES, 'lab': ASSIGNMENTS , 'tests/exam': TESTS, 'utm/life/other': GENERAL}\n",
    "\n",
    "csc108_fall2020_categories = {'general': GENERAL, 'administrative': GENERAL, 'post/uni-life': GENERAL, 'random': GENERAL, 'lecture': LECTURES, 'minor_labs': ASSIGNMENTS, 'major_lab1': ASSIGNMENTS, 'major_lab2': ASSIGNMENTS, 'major_lab3': ASSIGNMENTS, 'major_lab4': ASSIGNMENTS, 'major_lab5': ASSIGNMENTS, 'a1': ASSIGNMENTS, 'a2': ASSIGNMENTS, 'a3': ASSIGNMENTS, 'tests/exam': TESTS}\n",
    "\n",
    "csc108_fall2019_categories = {'general': GENERAL, 'utm/life/other': GENERAL, 'lecture': LECTURES, 'lab': ASSIGNMENTS, 'tests/exam': TESTS}\n",
    "\n",
    "ignore_categories = {'spatial', 'pcrs', 'pcrs/spatial_skills'}\n",
    "\n",
    "\n",
    "#augmented_df = augment_data(data, csc108_fall2021_categories, add_nlp_features=True, sentiment_model=sentiment_pipeline)\n",
    "#augmented_data[0].shape\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train, val and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods and Sources of Noise in Data\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Feature</th>\n",
    "    <th>Extraction Method</th>\n",
    "    <th>Noise</th>\n",
    "    <th>in literature?</th>\n",
    "    <th>justification of choice</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>post id</td>\n",
    "    <td>0.52</td>\n",
    "    <td>0.13</td>\n",
    "    <td>0.13</td>\n",
    "    <td>0.13</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>is_private</td>\n",
    "    <td>0.52</td>\n",
    "    <td>0.13</td>\n",
    "    <td>0.13</td>\n",
    "    <td>0.13</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>category</td>\n",
    "    <td>0.52</td>\n",
    "    <td>0.13</td>\n",
    "    <td>0.13</td>\n",
    "    <td>0.13</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>is_question_references</td>\n",
    "    <td>0.95</td>\n",
    "    <td>0.78</td>\n",
    "    <td>0.13</td>\n",
    "    <td>0.13</td>\n",
    "  \n",
    " \n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Question length\n",
    "\n",
    "##### Method\n",
    "- strip html tags and take token length\n",
    "##### Noise\n",
    "\n",
    "- imgs not factored into question length. imgs make question short when it is in reality longer\n",
    "- question length = 0,1\n",
    "  - question included in subject. Concat subject+question?\n",
    "  - question inc. img/screenshot and actual question is in subject\n",
    "- can include code snippets which makes question really long\n",
    "\n",
    "\n",
    "#### Is question references\n",
    "- checks if question contains "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
